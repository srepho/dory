{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Tutorial: Programmatic LLM Optimization for Insurance\n",
    "\n",
    "This tutorial is a comprehensive guide to **DSPy**, Stanford's framework for \"programming - not prompting\" language models. DSPy treats prompts as optimizable programs, enabling systematic improvement of LLM behavior.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. DSPy core concepts: Signatures, Modules, and Optimizers\n",
    "2. Building Weather Verification with ReAct\n",
    "3. Building Eligibility Determination with ChainOfThought\n",
    "4. Prompt optimization with MIPRO, BootstrapFewShot, and GEPA\n",
    "5. Exporting optimized prompts to other frameworks\n",
    "6. MLFlow integration for experiment tracking\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- OpenAI API key (or Anthropic)\n",
    "- Understanding of ML concepts\n",
    "\n",
    "---\n",
    "\n",
    "## Why DSPy?\n",
    "\n",
    "| Traditional Prompting | DSPy Approach |\n",
    "|----------------------|---------------|\n",
    "| Manual prompt engineering | Automated optimization |\n",
    "| Trial and error | Systematic metrics |\n",
    "| Prompts as strings | Prompts as programs |\n",
    "| Hard to maintain | Modular and composable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy httpx beautifulsoup4 python-dotenv mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_base = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n",
    "model_name = os.getenv(\"MODEL_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Configure DSPy\n",
    "lm = dspy.LM(\n",
    "    model=f\"openai/{model_name}\",\n",
    "    api_key=api_key,\n",
    "    api_base=api_base\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(\"DSPy configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Concepts\n",
    "\n",
    "### 2.1 Signatures\n",
    "\n",
    "Signatures define the input/output interface:\n",
    "\n",
    "```python\n",
    "class MySignature(dspy.Signature):\n",
    "    \"\"\"Docstring becomes the task description.\"\"\"\n",
    "    input_field: str = dspy.InputField(desc=\"What this input represents\")\n",
    "    output_field: str = dspy.OutputField(desc=\"What to generate\")\n",
    "```\n",
    "\n",
    "### 2.2 Modules\n",
    "\n",
    "Modules implement reasoning patterns:\n",
    "\n",
    "- `dspy.Predict`: Direct generation\n",
    "- `dspy.ChainOfThought`: Step-by-step reasoning\n",
    "- `dspy.ReAct`: Reasoning + Actions (tool use)\n",
    "\n",
    "### 2.3 Optimizers (Teleprompters)\n",
    "\n",
    "Optimizers improve prompts automatically:\n",
    "\n",
    "- `BootstrapFewShot`: Generate examples from training data\n",
    "- `MIPRO`: Multi-stage instruction optimization\n",
    "- `GEPA`: Genetic evolution of prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather Verification Signature\n",
    "class WeatherVerificationSignature(dspy.Signature):\n",
    "    \"\"\"Verify weather conditions for an insurance claim location.\"\"\"\n",
    "    \n",
    "    task: str = dspy.InputField(\n",
    "        desc=\"Task describing location and date to verify\"\n",
    "    )\n",
    "    \n",
    "    weather_report: str = dspy.OutputField(\n",
    "        desc=\"Structured weather report with location, coordinates, events, thunderstorm status, wind status\"\n",
    "    )\n",
    "\n",
    "# Eligibility Signature\n",
    "class EligibilitySignature(dspy.Signature):\n",
    "    \"\"\"Determine CAT event eligibility based on weather verification report.\"\"\"\n",
    "    \n",
    "    weather_report: str = dspy.InputField(\n",
    "        desc=\"Weather verification report with events and conditions\"\n",
    "    )\n",
    "    \n",
    "    decision: str = dspy.OutputField(\n",
    "        desc=\"One of: APPROVED, REVIEW, or DENIED\"\n",
    "    )\n",
    "    \n",
    "    reasoning: str = dspy.OutputField(\n",
    "        desc=\"Brief explanation for the decision\"\n",
    "    )\n",
    "    \n",
    "    confidence: str = dspy.OutputField(\n",
    "        desc=\"Confidence level: High, Medium, or Low\"\n",
    "    )\n",
    "\n",
    "print(\"Signatures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Tools for ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def geocode_location(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a location name to latitude/longitude coordinates.\n",
    "    Use this to get coordinates before fetching weather data.\n",
    "    \n",
    "    Args:\n",
    "        location: Address or place name (e.g., 'Brisbane, QLD')\n",
    "    \n",
    "    Returns:\n",
    "        String with location details and coordinates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with httpx.Client() as client:\n",
    "            r = client.get(\n",
    "                \"https://nominatim.openstreetmap.org/search\",\n",
    "                params={\"q\": f\"{location}, Australia\", \"format\": \"json\", \"limit\": 1},\n",
    "                headers={\"User-Agent\": \"InsuranceBot/1.0\"},\n",
    "                timeout=10.0\n",
    "            )\n",
    "            if r.status_code == 200 and r.json():\n",
    "                d = r.json()[0]\n",
    "                return f\"Location: {d['display_name']}\\nLatitude: {d['lat']}\\nLongitude: {d['lon']}\"\n",
    "            return \"Error: Location not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def get_bom_weather(latitude: str, longitude: str, date: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch weather observations from Australian Bureau of Meteorology.\n",
    "    \n",
    "    Args:\n",
    "        latitude: Latitude coordinate as string\n",
    "        longitude: Longitude coordinate as string\n",
    "        date: Date in YYYY-MM-DD format\n",
    "    \n",
    "    Returns:\n",
    "        Weather report with events found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        year, month, day = date.split(\"-\")\n",
    "        url = \"https://reg.bom.gov.au/cgi-bin/climate/storms/get_storms.py\"\n",
    "        params = {\n",
    "            \"begin_day\": day, \"begin_month\": month, \"begin_year\": year,\n",
    "            \"end_day\": day, \"end_month\": month, \"end_year\": year,\n",
    "            \"lat\": float(latitude), \"lng\": float(longitude),\n",
    "            \"event\": \"all\", \"distance_from_point\": \"50\", \"states\": \"all\"\n",
    "        }\n",
    "        \n",
    "        with httpx.Client() as client:\n",
    "            r = client.get(url, params=params, timeout=15.0)\n",
    "            if r.status_code != 200:\n",
    "                return f\"Error: HTTP {r.status_code}\"\n",
    "            \n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            events = []\n",
    "            for row in soup.find_all('tr')[1:]:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) >= 2:\n",
    "                    event = cells[0].get_text(strip=True)\n",
    "                    if event:\n",
    "                        events.append(event)\n",
    "            \n",
    "            has_thunder = any('thunder' in e.lower() or 'lightning' in e.lower() for e in events)\n",
    "            has_wind = any('wind' in e.lower() or 'gust' in e.lower() for e in events)\n",
    "            \n",
    "            return f\"\"\"Date: {date}\n",
    "Events: {', '.join(events) if events else 'None'}\n",
    "Has Thunderstorm: {has_thunder}\n",
    "Has Strong Wind: {has_wind}\"\"\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test\n",
    "print(geocode_location(\"Brisbane, QLD\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DSPy Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather Agent using ReAct\n",
    "weather_agent = dspy.ReAct(\n",
    "    signature=WeatherVerificationSignature,\n",
    "    tools=[geocode_location, get_bom_weather],\n",
    "    max_iters=5\n",
    ")\n",
    "\n",
    "# Test\n",
    "print(\"Testing Weather Agent...\")\n",
    "weather_result = weather_agent(task=\"Verify weather for Brisbane, QLD on 2025-03-07\")\n",
    "print(f\"\\nWeather Report:\\n{weather_result.weather_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eligibility Agent using ChainOfThought\n",
    "eligibility_agent = dspy.ChainOfThought(EligibilitySignature)\n",
    "\n",
    "# Test\n",
    "print(\"Testing Eligibility Agent...\")\n",
    "elig_result = eligibility_agent(weather_report=weather_result.weather_report)\n",
    "print(f\"\\nDecision: {elig_result.decision}\")\n",
    "print(f\"Reasoning: {elig_result.reasoning}\")\n",
    "print(f\"Confidence: {elig_result.confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training examples for eligibility determination\n",
    "training_examples = [\n",
    "    dspy.Example(\n",
    "        weather_report=\"\"\"Location: Brisbane, Queensland, Australia\n",
    "Coordinates: -27.4698, 153.0251\n",
    "Date: 2025-03-07\n",
    "Events: Thunderstorm, Wind Gust 85km/h, Heavy Rain\n",
    "Has Thunderstorm: True\n",
    "Has Strong Wind: True\"\"\",\n",
    "        decision=\"APPROVED\",\n",
    "        reasoning=\"Both thunderstorm and strong wind confirmed at valid Australian location within last 90 days\",\n",
    "        confidence=\"High\"\n",
    "    ).with_inputs(\"weather_report\"),\n",
    "    \n",
    "    dspy.Example(\n",
    "        weather_report=\"\"\"Location: Sydney, New South Wales, Australia\n",
    "Coordinates: -33.8688, 151.2093\n",
    "Date: 2025-03-07\n",
    "Events: Light Rain, Overcast\n",
    "Has Thunderstorm: False\n",
    "Has Strong Wind: False\"\"\",\n",
    "        decision=\"DENIED\",\n",
    "        reasoning=\"No severe weather events detected - only light rain and overcast conditions\",\n",
    "        confidence=\"High\"\n",
    "    ).with_inputs(\"weather_report\"),\n",
    "    \n",
    "    dspy.Example(\n",
    "        weather_report=\"\"\"Location: Melbourne, Victoria, Australia\n",
    "Coordinates: -37.8136, 144.9631\n",
    "Date: 2025-03-07\n",
    "Events: Thunderstorm\n",
    "Has Thunderstorm: True\n",
    "Has Strong Wind: False\"\"\",\n",
    "        decision=\"REVIEW\",\n",
    "        reasoning=\"Only thunderstorm detected, strong wind not confirmed - needs manual review\",\n",
    "        confidence=\"Medium\"\n",
    "    ).with_inputs(\"weather_report\"),\n",
    "    \n",
    "    dspy.Example(\n",
    "        weather_report=\"\"\"Location: Perth, Western Australia, Australia\n",
    "Coordinates: -31.9505, 115.8605\n",
    "Date: 2025-01-15\n",
    "Events: Wind Gust 70km/h\n",
    "Has Thunderstorm: False\n",
    "Has Strong Wind: True\"\"\",\n",
    "        decision=\"REVIEW\",\n",
    "        reasoning=\"Only strong wind detected without thunderstorm - partial conditions met\",\n",
    "        confidence=\"Medium\"\n",
    "    ).with_inputs(\"weather_report\"),\n",
    "    \n",
    "    dspy.Example(\n",
    "        weather_report=\"\"\"Location: Darwin, Northern Territory, Australia\n",
    "Coordinates: -12.4634, 130.8456\n",
    "Date: 2025-02-20\n",
    "Events: Thunderstorm, Wind Gust 95km/h, Hail\n",
    "Has Thunderstorm: True\n",
    "Has Strong Wind: True\"\"\",\n",
    "        decision=\"APPROVED\",\n",
    "        reasoning=\"Severe weather confirmed with both thunderstorm and strong winds plus hail\",\n",
    "        confidence=\"High\"\n",
    "    ).with_inputs(\"weather_report\"),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(training_examples)} training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eligibility_metric(example, prediction, trace=None):\n",
    "    \"\"\"\n",
    "    Evaluate eligibility prediction.\n",
    "    \n",
    "    Returns score based on:\n",
    "    - Decision match (primary)\n",
    "    - Confidence appropriateness (secondary)\n",
    "    \"\"\"\n",
    "    # Check decision match\n",
    "    expected_decision = example.decision.upper().strip()\n",
    "    predicted_decision = prediction.decision.upper().strip()\n",
    "    \n",
    "    decision_match = expected_decision == predicted_decision\n",
    "    \n",
    "    if not decision_match:\n",
    "        return 0.0\n",
    "    \n",
    "    # Bonus for appropriate confidence\n",
    "    expected_conf = example.confidence.lower().strip()\n",
    "    predicted_conf = prediction.confidence.lower().strip()\n",
    "    \n",
    "    conf_match = expected_conf == predicted_conf\n",
    "    \n",
    "    return 1.0 if conf_match else 0.8\n",
    "\n",
    "# Test metric\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "evaluator = Evaluate(\n",
    "    devset=training_examples,\n",
    "    metric=eligibility_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_score = evaluator(eligibility_agent)\n",
    "print(f\"\\nBaseline Score: {baseline_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimize with BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# BootstrapFewShot: Generate examples from the model itself\n",
    "bootstrap_optimizer = BootstrapFewShot(\n",
    "    metric=eligibility_metric,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=3\n",
    ")\n",
    "\n",
    "# Compile\n",
    "print(\"Optimizing with BootstrapFewShot...\")\n",
    "optimized_bootstrap = bootstrap_optimizer.compile(\n",
    "    eligibility_agent,\n",
    "    trainset=training_examples\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "bootstrap_score = evaluator(optimized_bootstrap)\n",
    "print(f\"\\nBootstrap Score: {bootstrap_score}%\")\n",
    "print(f\"Improvement: {bootstrap_score - baseline_score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimize with MIPRO (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dspy.teleprompt import MIPRO\n",
    "    \n",
    "    # MIPRO: Multi-stage instruction optimization\n",
    "    mipro_optimizer = MIPRO(\n",
    "        metric=eligibility_metric,\n",
    "        num_candidates=3,\n",
    "        init_temperature=1.0\n",
    "    )\n",
    "    \n",
    "    print(\"Optimizing with MIPRO (this may take a while)...\")\n",
    "    optimized_mipro = mipro_optimizer.compile(\n",
    "        eligibility_agent,\n",
    "        trainset=training_examples,\n",
    "        num_batches=2\n",
    "    )\n",
    "    \n",
    "    mipro_score = evaluator(optimized_mipro)\n",
    "    print(f\"\\nMIPRO Score: {mipro_score}%\")\n",
    "except ImportError:\n",
    "    print(\"MIPRO not available in this DSPy version\")\n",
    "    optimized_mipro = optimized_bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Extract and Export Optimized Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_optimized_prompt(module, name=\"Eligibility\") -> str:\n",
    "    \"\"\"\n",
    "    Extract optimized prompt from DSPy module for use in other frameworks.\n",
    "    \"\"\"\n",
    "    prompt_parts = [f\"# {name} Agent (DSPy Optimized)\\n\"]\n",
    "    \n",
    "    # Get signature\n",
    "    sig = getattr(module, 'signature', None)\n",
    "    if sig is None and hasattr(module, 'predict'):\n",
    "        sig = module.predict.signature\n",
    "    \n",
    "    if sig and sig.__doc__:\n",
    "        prompt_parts.append(f\"## Task\\n{sig.__doc__}\\n\")\n",
    "    \n",
    "    # Get demos (few-shot examples)\n",
    "    demos = getattr(module, 'demos', [])\n",
    "    if demos:\n",
    "        prompt_parts.append(\"## Examples\\n\")\n",
    "        for i, demo in enumerate(demos, 1):\n",
    "            prompt_parts.append(f\"### Example {i}\")\n",
    "            for key, value in demo.items() if hasattr(demo, 'items') else vars(demo).items():\n",
    "                if not key.startswith('_'):\n",
    "                    val_str = str(value)[:200] + \"...\" if len(str(value)) > 200 else str(value)\n",
    "                    prompt_parts.append(f\"**{key}**: {val_str}\")\n",
    "            prompt_parts.append(\"\")\n",
    "    \n",
    "    # Get field descriptions\n",
    "    if sig:\n",
    "        prompt_parts.append(\"## Input Fields\")\n",
    "        for name, field in sig.input_fields.items():\n",
    "            desc = \"\"\n",
    "            if hasattr(field, 'json_schema_extra') and field.json_schema_extra:\n",
    "                desc = field.json_schema_extra.get('desc', '')\n",
    "            prompt_parts.append(f\"- **{name}**: {desc}\")\n",
    "        \n",
    "        prompt_parts.append(\"\\n## Output Fields\")\n",
    "        for name, field in sig.output_fields.items():\n",
    "            desc = \"\"\n",
    "            if hasattr(field, 'json_schema_extra') and field.json_schema_extra:\n",
    "                desc = field.json_schema_extra.get('desc', '')\n",
    "            prompt_parts.append(f\"- **{name}**: {desc}\")\n",
    "    \n",
    "    return \"\\n\".join(prompt_parts)\n",
    "\n",
    "# Extract\n",
    "optimized_prompt = extract_optimized_prompt(optimized_bootstrap)\n",
    "print(\"Extracted Optimized Prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(optimized_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized prompt for use in other frameworks\n",
    "def save_for_framework(prompt: str, framework: str):\n",
    "    \"\"\"\n",
    "    Format and save optimized prompt for specific framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    if framework == \"pydantic_ai\":\n",
    "        return f'''\"\"\"DSPy-optimized system prompt for Pydantic AI.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "{prompt}\n",
    "\n",
    "## Business Rules\n",
    "- APPROVED: Both thunderstorms AND strong winds in valid Australian location\n",
    "- REVIEW: Only one severe weather type detected\n",
    "- DENIED: No severe weather or outside Australia\n",
    "\"\"\"\n",
    "'''\n",
    "    \n",
    "    elif framework == \"langchain\":\n",
    "        return f'''from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# DSPy-optimized prompt template\n",
    "ELIGIBILITY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"{prompt}\"\"\"),\n",
    "    (\"human\", \"Determine eligibility for:\\\\n\\\\n{{weather_report}}\")\n",
    "])\n",
    "'''\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Generate for different frameworks\n",
    "pydantic_prompt = save_for_framework(optimized_prompt, \"pydantic_ai\")\n",
    "print(\"Pydantic AI format:\")\n",
    "print(pydantic_prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Claims Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_claim_dspy(location: str, date: str, use_optimized: bool = True):\n",
    "    \"\"\"\n",
    "    Process insurance claim using DSPy agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Processing: {location} on {date}\")\n",
    "    print(f\"Using {'optimized' if use_optimized else 'baseline'} model\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Weather verification with ReAct\n",
    "    print(\"\\n[Weather Agent] Verifying...\")\n",
    "    weather_result = weather_agent(task=f\"Verify weather for {location} on {date}\")\n",
    "    print(f\"Report: {weather_result.weather_report[:200]}...\")\n",
    "    \n",
    "    # Step 2: Eligibility with optimized module\n",
    "    print(\"\\n[Eligibility Agent] Determining...\")\n",
    "    agent = optimized_bootstrap if use_optimized else eligibility_agent\n",
    "    elig_result = agent(weather_report=weather_result.weather_report)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DECISION:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Decision: {elig_result.decision}\")\n",
    "    print(f\"Reasoning: {elig_result.reasoning}\")\n",
    "    print(f\"Confidence: {elig_result.confidence}\")\n",
    "    \n",
    "    return {\n",
    "        \"weather_report\": weather_result.weather_report,\n",
    "        \"decision\": elig_result.decision,\n",
    "        \"reasoning\": elig_result.reasoning,\n",
    "        \"confidence\": elig_result.confidence\n",
    "    }\n",
    "\n",
    "# Test\n",
    "result = process_claim_dspy(\"Brisbane, QLD\", \"2025-03-07\", use_optimized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. MLFlow Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_experiment(\"dspy-claims-optimization\")\n",
    "\n",
    "def run_tracked_claim(location: str, date: str, use_optimized: bool = True):\n",
    "    \"\"\"Run claim with MLFlow tracking.\"\"\"\n",
    "    \n",
    "    run_name = f\"dspy_{location.split(',')[0]}_{'opt' if use_optimized else 'base'}\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params({\n",
    "            \"framework\": \"dspy\",\n",
    "            \"model\": model_name,\n",
    "            \"location\": location,\n",
    "            \"date\": date,\n",
    "            \"use_optimized\": use_optimized,\n",
    "            \"optimizer\": \"BootstrapFewShot\" if use_optimized else \"None\"\n",
    "        })\n",
    "        \n",
    "        start = datetime.now()\n",
    "        result = process_claim_dspy(location, date, use_optimized)\n",
    "        duration = (datetime.now() - start).total_seconds()\n",
    "        \n",
    "        mlflow.log_metrics({\"duration_seconds\": duration})\n",
    "        mlflow.log_text(result[\"weather_report\"], \"weather_report.txt\")\n",
    "        mlflow.set_tags({\n",
    "            \"decision\": result[\"decision\"],\n",
    "            \"confidence\": result[\"confidence\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nLogged: {result['decision']}, {duration:.2f}s\")\n",
    "        return result\n",
    "\n",
    "# Run tracked\n",
    "run_tracked_claim(\"Brisbane, QLD\", \"2025-03-07\", use_optimized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log optimization results\n",
    "with mlflow.start_run(run_name=\"optimization_comparison\"):\n",
    "    mlflow.log_params({\n",
    "        \"optimizer\": \"BootstrapFewShot\",\n",
    "        \"training_examples\": len(training_examples),\n",
    "        \"max_bootstrapped_demos\": 3\n",
    "    })\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"baseline_score\": baseline_score,\n",
    "        \"optimized_score\": bootstrap_score,\n",
    "        \"improvement\": bootstrap_score - baseline_score\n",
    "    })\n",
    "    \n",
    "    mlflow.log_text(optimized_prompt, \"optimized_prompt.md\")\n",
    "    \n",
    "    print(\"Optimization comparison logged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs optimized\n",
    "print(\"Comparing baseline vs optimized...\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"Brisbane, QLD\", \"2025-03-07\"),\n",
    "    (\"Sydney, NSW\", \"2025-03-07\"),\n",
    "]\n",
    "\n",
    "for loc, dt in test_cases:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {loc}\")\n",
    "    \n",
    "    # Baseline\n",
    "    base = run_tracked_claim(loc, dt, use_optimized=False)\n",
    "    \n",
    "    # Optimized  \n",
    "    opt = run_tracked_claim(loc, dt, use_optimized=True)\n",
    "    \n",
    "    print(f\"\\nBaseline: {base['decision']}\")\n",
    "    print(f\"Optimized: {opt['decision']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View experiment\n",
    "exp = mlflow.get_experiment_by_name(\"dspy-claims-optimization\")\n",
    "runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "\n",
    "print(\"\\nExperiment Runs:\")\n",
    "cols = ['run_id', 'params.location', 'params.use_optimized', 'metrics.duration_seconds', 'tags.decision']\n",
    "cols = [c for c in cols if c in runs.columns]\n",
    "print(runs[cols].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Key Takeaways\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **DSPy Signatures**: Declarative input/output specifications\n",
    "2. **DSPy Modules**: ChainOfThought, ReAct for different reasoning patterns\n",
    "3. **Tools**: Integrating Python functions with ReAct agents\n",
    "4. **Optimization**: BootstrapFewShot for automatic prompt improvement\n",
    "5. **Export**: Extracting prompts for use in other frameworks\n",
    "6. **MLFlow**: Tracking experiments and comparing results\n",
    "\n",
    "### DSPy Strengths\n",
    "\n",
    "- **Systematic optimization** - no more manual prompt tweaking\n",
    "- **Composable modules** - build complex pipelines from simple parts\n",
    "- **Portable prompts** - export optimized prompts to any framework\n",
    "- **Metrics-driven** - optimize based on actual performance\n",
    "\n",
    "### DSPy Challenges\n",
    "\n",
    "- Learning curve (different paradigm)\n",
    "- Less control over exact prompt text\n",
    "- Optimization requires good training data\n",
    "\n",
    "### For Insurance Teams\n",
    "\n",
    "- **Use DSPy when**: You need to optimize prompts systematically\n",
    "- **Combine with**: Other frameworks for deployment (Pydantic AI for production)\n",
    "- **Best practice**: Optimize with DSPy, export to your production framework\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Add more training examples\n",
    "2. Try different optimizers (MIPRO, GEPA)\n",
    "3. Export optimized prompts to Pydantic AI\n",
    "4. Set up automated optimization pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tutorial complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
